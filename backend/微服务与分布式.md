# 分布式

## 一致性 Hash 算法

**普通 Hash 的问题**

服务器数量变动的时候，所有缓存的位置都要发生改变

当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端数据库请求数据，造成 DB 负载 ↑



**一致性 Hash 算法**

致性Hash算法是对2^32取模，将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-232-1Hash 环： 整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到232-1，也就是说0点左侧的第一个点代表232-1， 0和232-1在零点中方向重合，我们把这个由232个点组成的圆环称为Hash环。

节点： 将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器！



**特性-容错性**

现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。

在某一台机器宕机的情况下，只影响宕机服务器到其环空间中前一台服务器之间的数据



**特性-拓展性**

在系统中增加一台服务器Node X

此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X ！一般的，在一致性Hash算法中，如果增加一台服务器，则受影响的数据仅仅是 <u>新服务器到其环空间中前一台服务器</u> （即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。

节点 C 宕机，影响到原来 B-C 之间的数据，使其指向 Node D；

新增 Node X, 只影响 B-X 之间的数据，使得散落在这之间的 hash 桶指向到 Node X；



**Hash 环的数据倾斜问题**

一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成**数据倾斜**（**被缓存的对象大部分集中缓存在某一台服务器上**）问题，例如系统中只有两台服务器，其环分布如下



虚拟节点： 对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。





实现： 

可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点。

在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。



## Raft 一致性算法

Docker Swarm 底层实现分布式一致性



## CAP

CAP基本概念
C：一致性，就是说所有的服务器上面的数据都是一样的，也是数据一致更新的意思，所有数据变动都是同步的，是一种强一致性；
A：可用性，用户访问服务器上面的数据，响应时间在可以接受的范围内；
P：分区容忍性，其实就是高可用性，一个节点崩了，并不影响我们其它的节点；



## 分布式锁

常见的三种实现方式

**DB 乐观锁**



**Zookeeper 实现** 



**Redis 实现** 

通过 Redis 中的 setnx 和 expire 实现分布式锁；

借助 Lua 脚本控制实现



## 分布式ID生成方案

(1) UUID：不适合作为主键，因为太⻓了，并且⽆序不可读，查询效率低。⽐较适合⽤于⽣成唯⼀的 名字的标示⽐如⽂件的名字。 

(2) 数据库⾃增 id : 两台数据库分别设置不同步⻓，⽣成不重复ID的策略来实现⾼可⽤。这种⽅式 ⽣成的 id 有序，但是需要独⽴部署数据库实例，成本⾼，还会有性能瓶颈。

(3) 利⽤ redis ⽣成 id : 性能⽐较好，灵活⽅便，不依赖于数据库。但是，引⼊了新的组件造成 系统更加复杂，可⽤性降低，编码更加复杂，增加了系统成本。 

(4) Twitter的snowflake算法 ： 

(5) 美团的Leaf分布式ID⽣成系统 ：Leaf 是美团开源的分布式ID⽣成器，能保证全局唯⼀性、趋势 递增、单调递增、信息安全。



SnowFlake： 整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID作区分)，效率较高，经测试，SnowFlake每秒能够产生26万ID左右;





基于独立数据库实现：

Rumba-Sequence

通过数据库中的表控制，支持缓存提高性能，借助特定的命名规则控制作用的域



## 网站高并发

**海量数据的解决方案：**

1. 使用缓存；

2. 页面静态化技术；

3. 数据库优化；

4. 分离数据库中活跃的数据；

5. 批量读取和延迟修改；

6. 读写分离；

7. 使用NoSQL和Hadoop等技术；

8. 分布式部署数据库；

9. 应用服务和数据服务分离；

10. 使用搜索引擎搜索数据库中的数据；

11. 进行业务的拆分；

    

**高并发情况下的解决方案：**

1. 应用程序和静态资源文件进行分离；
2. 页面缓存；
3. 集群与分布式；
4. 反向代理；
5. CDN；



## **Session** **管理**

Session Sticky:

> 粘滞会话，与对应的服务器绑定，容灾、固定路径    

​    |-application server 重启 session 全部消失

​    |-load balancer 成了一个有状态的机器，容灾麻烦

对于同一连接的数据包会对其进行 NAT 转化后，将其转化到后端固定的服务器处理。

解决了 Session 共享的问题



Session Replication：  

> session复制、同步session、内存问题

​    |-适合机器不多的情况

​    |-应用服务器之间的带宽问题，不停的同步 session 

​    |- 大量 user 在线，占用 memory 过多

后台的服务器进行 Session 的共享，





Cookie with session：    

> 带有session信息、长度有限、安全性

Cookie 安全性问题，且有长度限制





Session 数据集中存储：

> 单点问题、可用性   session集群

适用于 session 数量及 web 应用服务器多的情况

写应用时要调整写 session 的业务逻辑

 需要解决 session server 单点，提高可用性





## 限流算法

> 通过限流，我们可以很好地控制系统的qps，从而达到保护系统的目的。

**计数器法**

> 存在在间隔点的两次瞬间达到最大并发

（1） 算法实现

计数器法是限流算法里最简单也是最容易实现的一种算法。比如我们规定，对于A接口来说，我们1分钟的访问次数不能超过100个。那么我们可以这么做：在一开始的时候，我们可以设置一个计数器counter，每当一个请求过来的时候，counter就加1，如果counter的值大于100并且该请求与第一个请求的间隔时间还在1分钟之内，那么说明请求数过多；如果该请求与第一个请求的间隔时间大于1分钟，且counter的值还在限流范围内，那么就重置counter，具体算法的示意图



（2） 临界问题

这个算法虽然简单，但是有一个十分致命的问题，那就是临界问题

从上图中我们可以看到，假设有一个恶意用户，他在0:59时，瞬间发送了100个请求，并且1:00又瞬间发送了100个请求，那么其实这个用户在1秒里面，瞬间发送了200个请求。我们刚才规定的是1分钟最多100个请求，也就是每秒钟最多1.7个请求，用户通过在时间窗口的重置节点处突发请求，可以瞬间超过我们的速率限制。用户有可能通过算法的这个漏洞，瞬间压垮我们的应用。

聪明的朋友可能已经看出来了，刚才的问题其实是因为我们统计的精度太低。那么如何很好地处理这个问题呢？或者说，如何将临界问题的影响降低呢？我们可以看下面的滑动窗口算法。

// todo 伪代码实现



**滑动窗口法**

> 将窗口划分为多个计数器

（1）介绍

rolling window。为了解决这个问题，我们引入了滑动窗口算法。

（2） 执行逻辑

在上图中，整个红色的矩形框表示一个时间窗口，在我们的例子中，一个时间窗口就是一分钟。然后我们  <u>将时间窗口进行划分</u>  ，比如图中，我们就将滑动窗口划成了6格，所以每格代表的是10秒钟。每过10秒钟，我们的时间窗口就会往右滑动一格。每一个格子都有自己独立的计数器counter，比如当一个请求在0:35秒的时候到达，那么0:30~0:39对应的counter就会加1。

那么滑动窗口怎么解决刚才的临界问题的呢？我们可以看上图，0:59到达的100个请求会落在灰色的格子中，而1:00到达的请求会落在橘黄色的格子中。当时间到达1:00时，我们的窗口会往右移动一格，那么此时时间窗口内的总请求数量一共是200个，超过了限定的100个，所以此时能够检测出来触发了限流。

（3） 与计数器算法比较

计数器算法其实就是滑动窗口算法。只是它没有对时间窗口做进一步地划分，所以只有1格。

由此可见，当滑动窗口的格子划分的越多，那么滑动窗口的滚动就越平滑，限流的统计就会越精确。



**漏桶算法**

> 又称leaky bucket





**令牌桶算法**

> 又称token bucket

（2） 执行逻辑

从图中我们可以看到，令牌桶算法比漏桶算法稍显复杂。首先，我们有一个 固定容量的桶，桶里存放着令牌（token）。桶一开始是空的，token以一个固定的速率r往桶里填充，直到达到桶的容量，多余的令牌将会被丢弃。每当一个请求过来时，就会尝试从桶里移除一个令牌，如果没有令牌的话，请求无法通过。

// todo 具体的伪代码实现如下：

（3） 令牌桶算法的变种

若仔细研究算法，我们会发现我们默认从桶里移除令牌是不需要耗费时间的。如果给移除令牌设置一个延时时间，那么实际上又采用了漏桶算法的思路。Google的guava库下的SmoothWarmingUp类就采用了这个思路。

（4） 临界问题的处理

我们再来考虑一下临界问题的场景。在0:59秒的时候，由于桶内积满了100个token，所以这100个请求可以瞬间通过。但是由于token是以较低的速率填充的，所以在1:00的时候，桶内的token数量不可能达到100个，那么此时不可能再有100个请求通过。所以令牌桶算法可以很好地解决临界问题。下图比较了计数器（左）和令牌桶算法（右）在临界点的速率变化。我们可以看到虽然令牌桶算法允许突发速率，但是下一个突发速率必须要等桶内有足够的token后才能发生：



**算法比较**

（1） 计数器 VS 滑动窗口
计数器算法是最简单的算法，可以看成是滑动窗口的低精度实现。滑动窗口由于需要存储多份的计数器（每一个格子存一份），所以滑动窗口在实现上需要更多的存储空间。也就是说，如果 *滑动窗口的精度越高，需要的存储空间就越大* 。

（2） 漏桶算法和令牌桶算法最明显的区别是 *令牌桶算法允许流量一定程度的突发*。因为默认的令牌桶算法，取走token是不需要耗费时间的，也就是说，假设桶内有100个token时，那么可以瞬间允许100个请求通过。

令牌桶算法由于实现简单，且允许某些流量的突发，对用户友好，所以被业界采用地较多。当然我们需要具体情况具体分析，只有最合适的算法，没有最优的算法。



## 负载均衡

轮询： 不考虑server能力

权重：考虑server的处理能力，对每台服务器给出一个权重

地址散列： 同一个用户访问同一个server

​    |-源ip地址散列

​    |-目标Ip地址散列

最小连接：     集群中各个服务器负载更加均匀

加权最小连接：

​    |-算法：(活动连接数*256 + 非活动连接数) / 权重   

​    其中值较小的server优先被选中

 



Haproxy

TCP 四层、HTTP 七层应用的代理软件

树形存储...







keepalive:

负载均衡组件的高可用





// todo etcd 分布式存储

// todo gossip 协议





# SpringCloud

## 基础

### 服务拆分

业务形态不适合的 

- 系统中包含很多**很多强事务场景**的 
- 业务相对稳定，迭代周期长 
- 访问压力不大，可用性要求不高



服务端发现

代理

Nginx 
Zookeeper 
Kubernetes



客户端发现(Eureka)

无需代理 
需要知道所有的。。。

eureka 使用







**康威定律**

任何组织在设计一套系统（广义概念上的系统）时，所交付的设计方案在结构上都与该组织的沟通结构保持一致。

团队结构、运作管理

团队管理结构

微服务的特点 
·一系列微小的服务共同组成 
·单独部署，跑在自己的进程里 
·每个服务为独立的业务开发 
·分布式的管理

人员组织：

按照终端分开、按照业务分开

服务和数据的关系 
 先考虑业务功能，再考虑数据 
 无状态服务

任何组织在设计一套系统（广义概念上的系统）时，所交付的设计方案在结构上都与该组织的沟通结构保持一致。

团队结构、运作管理

团队管理结构

微服务的特点 
·一系列微小的服务共同组成 
·单独部署，跑在自己的进程里 
·每个服务为独立的业务开发 
·分布式的管理

人员组织：

按照终端分开、按照业务分开

服务和数据的关系 
◆先考虑业务功能，再考虑数据 
◆无状态服务



## Feign



使用特定的 RequestTemplate 作为 RequestInterceptor 的放啊参数

通过 Feign 调用的时候将请求头 `trace_id` 放入，值为当前线程对应的追踪 ID

对当前登录的 key 进行





分布式/版本化配置

全局锁??

分布式消息投递

服务发现

负载均衡

断路器





hystrix dashboard



```
spring cloud --list
# 加解密
spring encrypt mysecret --key foo
spring encrypt mysecret --key @${HOME}/.ssh/id_rsa.pub
```





`/META-INFO/spring.factories`: 配置属性为 `org.springframework.cloud.BootstrapConfiguration` 类中的 key

通过 PropertySourceLocator 接口定制化个性化配置 PropertySource

```
org.springframework.cloud.bootstrap.BootstrapConfiguration=sample.custom.CustomPropertySourceLocator
```





Refresh Scope:

`@Bean` 配置了 `RefreshScope` 将在配置更改的时候会重新初始化

是在使用时初始化的懒惰代理，强制 bean 重新初始化调用只需要使其 cache entry 失效

配置了 `RefreshScope` 提供公共的 `refreshAll()` 方法，刷新所有在其中的 bean，注解工作于 `@Configuration` 类上





```
@EnableDiscoveryClient(autoRegister = false)
```





使用 RestTemplate 作为负载均衡， `@LoadBalanced` 





忽视网络接口:

忽视特定命名的网络接口，从服务发现注册中排除，如排除在容器中运行的网络

强制使用特定的网络地址，使用正则表达式进行匹配

强制值使用本地的地址

```yml
spring:
  cloud:
    inetutils:
      ignoredInterfaces:
        - docker0
        - veth.*
```







(2) 启动主类添加： 
@EnableFeignClients

(3) 声明方法 
@FeignClient(name="product") 
代表访问的是某个服务的接口

访问的是客户端

声明式REST客户端（伪RPC） 
采用了基于接口的注解

内部使用 Ribbon 做负载均衡



## Eureka

提供元数据:

host

port

health indicator URL

home page

<p align="center">Eureka 重要配置</p>

| 配置                                                 | 说明                       | 备注 |
| ---------------------------------------------------- | -------------------------- | ---- |
| eureka.client.serviceUrl.defaultZone                 |                            |      |
| eureka.instance.lease-renewal-interval-in-seconds    |                            |      |
| eureka.instance.instance-id                          | 显示在 Eureka 的实例 ID    |      |
| eureka.instance.lease-expiration-duration-in-seconds | 30，实例无效后多长时间移除 |      |
| eureka.client.register-with-eureka                   | Ture, 注册到 Eureka        |      |
| eureka.client.fetch-registry                         | True, 获取 Eureka 注册内容 |      |



### client

在超过配置的事件 heartbeat 失败后，该实例自动从注册中移除



当 `spring-cloud-starter-netflix-eureka-client` 在类路劲下时，应用自动注册到 Eureka Server，配置中指明 Eureka server， `eureka.client.serviceUrl.defaultZone` 为服务注册的默认地址，对于表示首选项的客户端此配置无效



设置 `eureka.client.enabled` 为 false 时，服务发现将自动失效



配置 `eureka.client.serviceUrl.defaultZone` 的 URL 支持特定的用户名和密码

```
user@password@172.17.10.150:8761/eureka
```



默认的状态页和健康指标为 `/info` 和 `/health`，默认是 SB 的 Actuator 应用，通过指定 Instance 下的 statusPageUrlPath 和 healthCheckUrlPath 进行指定





默认使用 30 秒进行 heartbeat 去注册，instance、server、client 都有相同的元数据在他们的本地缓冲中时



对应的类

```java
@Autowired
private DiscoveryClient discoveryClient;

public String serviceUrl() {
    List<ServiceInstance> list = discoveryClient.getInstances("STORES");
    if (list != null && list.size() > 0 ) {
        return list.get(0).getUri();
    }
    return null;
}
```









### server

包含 Eureka server，使用 `spring-cloud-starter-netflix-eureka-server`，在对应的启动类上增加 `@EnableEurekaServer` 注解进行配置







安全性:

`spring-boot-starter-security` 进行设置到类路径下，需要一个 CSRF 的 token





## Zuul

基于 JVM 的路由器，服务端的负载均衡器

认证

压力测试

减载



规则引擎让规则和过滤器基本上以任何 JVM 语言变现，内嵌支持 Java、Groovy



`@EnableZuulProxy`: 反向代理，前缀剥离，使用 Ribbon 找到通过转发到对应的 instanc



整合了 Ribbon 以及 Hystrix



### 过滤器

进行 Routing 的过滤器





### 管理端点

`@EnableZuulProxy` 与 Spring Boot Actuator 配合时，暴力 /routes 和 filters



/routes: GET 和 POST 请求不同的逻辑， `/routes?format=details 查看更多的详情





配置请求失败后的重试策略，包括 rebion、restTemplate、zuul ，`LoadBalancedRetryFactory` 覆盖 `createBackOffPolicy` 方法，使用 Ribbon，可以挺过配置配置特定的 Ribbon 属性

```
client.ribbon.MaxAutoRetries
client.ribbon.MaxAutoRetriesNextServer
client.ribbon.OkToRetryOnAllOperations
client.ribbon.retryableStatusCodes: 404,502
```

```
# 关闭 zuul 重试策略
zuul.retryable=false
zuul.routesl.routename.retryable=false
```



**zuul 的超时**

Zuul 转发使用 RibbonRoutingFilter 过滤器，整合了 Hystrix 和 Ribbot



直接配置 URL 路由，用不上 Ribbon、Hystrix，使用到的过滤器为 SimepleHostRoutingFilter



Resouces

调优

**Tomcat 参数**

```yaml
server:
  tomcat:
    max-connections:  
    max-thread:
    min-spare-thread: # 最小空闲线程数
    accept-count: # tomcat 启动线程达到 max，接收派对的请求个数
```





**Feign 参数**

默认使用 JDK URLConnection 发送请求，无连接池，可考虑使用 HttpClient 或 OKHttp 作为连接池

```yaml
feign:
  httpclient:
    enable: true
    max-connections: 200
    max-connections-per-route: 50
```

```yaml
feign:
  okhttp:
    enabled: true
  httpclient:
    max-connections: 200
    max-connecitno-per-route: 50
```



**ZUUL**

zuul 的隔离策略默认为 SEMAPHORE

```yaml
zuul:
  semaphore:
    max-semaphores: 100
```





### hystrix

> 

```YML
hystrix:
 command:
  default:
   execution:
    timeout:
     enabled: true
    isolation:
     thread:
      timeoutInMilliseconds: 280000   
```





### Ribbon

> 客户端负载均衡器：Ribbon。

```yml
ribbon:
 ReadTimeout: 60000
 ConnectTimeout: 10000
 MaxAutoRetries: 1                # 同一台实例最大重试次数,不包括首次调用
 MaxAutoRetriesNextServer: 1      # 重试负载均衡其他的实例最大重试次数,不包括首次调用
 OkToRetryOnAllOperations: false  # 是否所有操作都重试
```





[Eureka 配置](https:/github.com/Netflix/eureka/wiki/Configuring-Eureka)



